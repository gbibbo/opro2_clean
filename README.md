# OPRO2 - Optimizaci√≥n de Prompts para Detecci√≥n de Habla

Pipeline completo de optimizaci√≥n de prompts (OPRO) para detecci√≥n de habla con Qwen2-Audio y LoRA en Surrey HPC.

---

## Descripci√≥n

Este repositorio implementa un pipeline de 7 etapas para optimizar la detecci√≥n de habla mediante:
- **OPRO (Optimization by PROmpting)**: Optimizaci√≥n autom√°tica de prompts usando un LLM local
- **LoRA (Low-Rank Adaptation)**: Fine-tuning eficiente del modelo Qwen2-Audio-7B-Instruct
- **Evaluaci√≥n psicoac√∫stica**: Medici√≥n de rendimiento bajo 22 condiciones independientes

---

## √öltimos Resultados Experimentales

### ‚ö†Ô∏è Hallazgo Cr√≠tico: Modelo BASE Sin Fine-tuning NO es Viable (26 diciembre 2025)

**Diagn√≥stico completo revela sesgo inherente del modelo BASE:**

El modelo Qwen2-Audio-7B-Instruct **sin fine-tuning presenta un sesgo cr√≠tico hacia clasificar audios como SPEECH**, independientemente de la estrategia de prompting utilizada.

**Evidencia experimental (100 samples NONSPEECH):**
- ‚úÖ **Correctos (NONSPEECH)**: 67%
- ‚ùå **Incorrectos (predice SPEECH)**: 33%
- Con **degradaciones severas** (test set completo): Solo **25.52% accuracy** en NONSPEECH

**Observaciones t√©cnicas:**
- El modelo responde literalmente `"SPEECH"` cuando deber√≠a decir `"NONSPEECH"`
- `confidence=1.0` siempre (muy seguro, aunque est√© equivocado)
- Latencia m√°s baja para respuestas "SPEECH" (~177ms) vs "NONSPEECH" (~204ms)
- El sistema de normalizaci√≥n funciona perfectamente - el problema est√° en el modelo, no en el post-procesamiento

**Conclusi√≥n:** El fine-tuning con LoRA es **esencial** para esta tarea. No se recomienda optimizar prompts para el modelo BASE sin LoRA.

---

### Tabla Comparativa Completa de Configuraciones

**Test Set:** 21,340 muestras | **Seed:** 42 | **Fecha:** 15-26 diciembre 2025

| Configuraci√≥n | BA_clip | BA_conditions | Speech Acc | Nonspeech Acc | Notas |
|--------------|---------|---------------|------------|---------------|-------|
| **1. BASE + OPRO auto (20 samples)** | **59.89%** ‚ùå | - | **94.26%** ‚ö†Ô∏è | **25.52%** ‚ùå | Sesgo cr√≠tico hacia SPEECH |
| **2. BASE + OPRO Cl√°sico** | 88.12% | 89.34% | 91.64% | 84.60% | Resultado previo ¬π |
| **3. BASE + OPRO Varied (30 samples)** | **59.89%** ‚ùå | - | **94.26%** ‚ö†Ô∏è | **25.52%** ‚ùå | Mismo sesgo que #1 |
| **4. LoRA + OPRO Cl√°sico** | **94.90%** ‚≠ê | **95.46%** ‚≠ê | **98.23%** | **91.57%** | Mejor resultado general |
| **5. LoRA + OPRO Open** | **94.78%** ‚úÖ | **95.32%** ‚úÖ | **98.23%** | **91.34%** | Resultado previo ¬≤ |
| **6. LoRA + OPRO Varied (30 samples)** | **92.99%** ‚úÖ | - | **98.35%** | **87.64%** | Prompts m√°s diversos |

**Observaciones clave:**
- **BASE sin LoRA (Configs 1-3):** Resultados inutilizables debido a sesgo inherente
  - Sobre-predice SPEECH (94% correcto)
  - Falla dram√°ticamente en NONSPEECH (25% correcto)
  - BA resultante: ~60% (peor que random guessing en clases balanceadas)
- **LoRA funciona bien (Configs 4-6):** Corrige el sesgo completamente
  - **BASE ‚Üí LoRA:** +35% en NONSPEECH accuracy, +33% en BA
  - OPRO Varied con m√°s diversidad: 92.99% BA (excelente, aunque 2% menor que Cl√°sico)
- **Mejor configuraci√≥n:** LoRA + OPRO Cl√°sico con **94.90% BA**

---

### Prompts Optimizados por OPRO

**¬π Mejor Prompt - BASE + OPRO Cl√°sico (resultado previo v√°lido):**
```
Listen briefly; is this clip human speech or noise? Quickly reply: SPEECH or NON-SPEECH.
```
*Nota: Este resultado proviene de un experimento anterior. Los experimentos recientes muestran que BASE sin LoRA tiene sesgo cr√≠tico.*

**Prompts de BASE con sesgo (Configs 1 y 3):**
```
Classify this audio. Output only: SPEECH or NONSPEECH.
```
*Este prompt obtiene 59.89% BA debido al sesgo del modelo BASE, NO por la calidad del prompt.*

**¬≤ Mejor Prompt - LoRA + OPRO Cl√°sico (Config 4 - MEJOR GENERAL):**
```
Decide the dominant content.
Definitions:
- SPEECH = human voice, spoken words, syllables, conversational cues.
- NONSPEECH = music, tones/beeps, environmental noise, silence.
Output exactly: SPEECH or NONSPEECH.
```

**Mejor Prompt - LoRA + OPRO Open (Config 5):**
```
Decide the dominant content.
Definitions:
- SPEECH = human voice, spoken words, syllables, conversational cues.
- NONSPEECH = music, tones/beeps, environmental noise, silence.
Output exactly: SPEECH or NONSPEECH.
```

**Mejor Prompt - LoRA + OPRO Varied (Config 6 - 30 samples, 15 seed prompts diversos):**
```
Does this audio contain human speech? Answer SPEECH or NONSPEECH.
```

**Observaciones:**
- OPRO Cl√°sico y OPRO Open convergieron al **mismo prompt id√©ntico** con definiciones expl√≠citas
- OPRO Varied (con seeds m√°s diversos) encontr√≥ un prompt m√°s simple pero igualmente efectivo
- La diferencia de rendimiento (94.90% vs 92.99%) puede deberse a la naturaleza m√°s directa del prompt sin definiciones

---

### Seed Prompts del Experimento OPRO Varied

El experimento OPRO Varied (Configs 3 y 6) utiliz√≥ **15 seed prompts diversos** dise√±ados para explorar diferentes estrategias de prompting:

**Prompts Descriptivos Abiertos:**
1. "What do you hear in this audio?"
2. "Describe what you hear in this audio clip."
3. "What is the primary sound source in this audio clip?"

**Prompts Binarios Directos:**
4. "Does this audio contain human speech? Answer SPEECH or NONSPEECH."
5. "Classify this audio. Output only: SPEECH or NONSPEECH."
6. "Is there human speech in this recording? Reply with one word: SPEECH or NONSPEECH."

**Prompts con Definiciones:**
7. "Listen carefully. SPEECH means human voice or talking. NONSPEECH means music, noise, or silence. What is this?"
8. "Decide the dominant content. If human voice is present, say SPEECH. Otherwise, say NONSPEECH."

**Prompts con Ejemplos (Few-shot):**
9. "Example: beeping sounds ‚Üí NONSPEECH. Example: person talking ‚Üí SPEECH. Now classify this audio:"

**Prompts T√©cnicos:**
10. "Audio classification task. Detect if human vocal tract sounds are present. Answer: SPEECH or NONSPEECH."
11. "Analyze the acoustic content. If you identify human voice, speaking, or conversation, output SPEECH. For music, tones, noise, or silence, output NONSPEECH."

**Formatos Alternativos:**
12. "Listen. Does this contain: A) Human speech, or B) Other sounds? Output A or B."
13. "Speech detection: YES if human voice detected, NO otherwise."
14. "TASK: Binary classification. LABELS: SPEECH (human voice) or NONSPEECH (all other sounds). AUDIO:"
15. "Quick check: human speech present? SPEECH or NONSPEECH."

**Resultado:** A pesar de la gran diversidad de estrategias, OPRO convergi√≥ a un prompt efectivo simple: *"Does this audio contain human speech? Answer SPEECH or NONSPEECH."* (92.99% BA en LoRA).

---

### An√°lisis Detallado: LoRA + OPRO Cl√°sico (Mejor Resultado)

**Desglose por Dimensi√≥n Psicoac√∫stica:**
| Dimensi√≥n | BA | Condiciones |
|-----------|-----|------------|
| Duration | 89.14% | 8 condiciones (20ms-1000ms) |
| SNR | 97.11% | 6 condiciones (-10dB a 20dB) |
| Reverb | 93.71% | 4 condiciones (none, 0.3s, 1.0s, 2.5s) |
| Filter | 93.94% | 4 condiciones (none, bandpass, lowpass, highpass) |

**Rendimiento por Condici√≥n (Top 5):**
1. SNR 5dB: 97.32% BA
2. SNR 10dB: 97.22% BA
3. SNR -10dB/20dB/-5dB/0dB: ~97.01% BA
4. Filter Bandpass: 94.33% BA
5. Filter Lowpass: 94.12% BA

**Rendimiento por Condici√≥n (Bottom 5):**
1. Duration 20ms: 80.93% BA
2. Duration 40ms: 84.85% BA
3. Duration 60ms: 87.32% BA
4. Duration 80ms: 88.04% BA
5. Duration 100ms: 90.82% BA

**Evoluci√≥n del OPRO:**
- Iteraci√≥n 1: 90% accuracy (prompt inicial)
- Mejor iteraci√≥n: Iter 76 con 100% accuracy en muestra de validaci√≥n
- Total de prompts generados: 121 (15 iteraciones √ó 8 candidatos)
- Top prompts recurrentes:
  - "Does this audio contain human speech? Answer exactly one token: SPEECH or NONSPEECH." ‚Üí 95%
  - "Decide the dominant content..." ‚Üí 100% (mejor)
  - "Label SPEECH only if human voice is clearly present..." ‚Üí 95%

---

### Conclusiones

1. **LoRA es CR√çTICO para esta tarea:** El modelo BASE sin fine-tuning presenta sesgo inherente hacia SPEECH
   - BASE: 59.89% BA (25.52% NONSPEECH accuracy) ‚ùå
   - LoRA: 92.99-94.90% BA (87.64-91.57% NONSPEECH accuracy) ‚úÖ
   - **Mejora: +35 puntos porcentuales** en detecci√≥n de NONSPEECH

2. **El sesgo del modelo BASE NO se puede corregir con prompting:**
   - Probamos 3 estrategias diferentes de OPRO (auto, cl√°sico, varied)
   - Todas obtienen resultados similares (~60% BA)
   - El modelo responde literalmente "SPEECH" cuando deber√≠a decir "NONSPEECH"
   - Diagn√≥stico con 100 samples: 33% de error en casos limpios de NONSPEECH

3. **OPRO funciona excelentemente CON LoRA:**
   - LoRA + OPRO Cl√°sico: **94.90% BA** ‚≠ê
   - LoRA + OPRO Open: 94.78% BA ‚úÖ
   - LoRA + OPRO Varied: 92.99% BA ‚úÖ
   - Diferentes estrategias de prompting convergen a resultados similares (92-95% BA)

4. **Diversidad de seed prompts no garantiza mejor rendimiento:**
   - OPRO Cl√°sico (8 seeds similares): 94.90% BA
   - OPRO Varied (15 seeds diversos): 92.99% BA
   - La optimizaci√≥n converge a prompts efectivos independientemente de las semillas

5. **Desaf√≠os t√©cnicos identificados:**
   - **Duraci√≥n corta (<100ms):** Peor rendimiento (80-90% BA)
   - **SNR muy robusto:** Excelente incluso a -10dB (97% BA)
   - **Infraestructura:** Nodos aisurrey14/aisurrey19 tienen problemas de CUDA

---

### Diagn√≥stico del Sesgo del Modelo BASE

Para entender exactamente por qu√© el modelo BASE falla, ejecutamos un diagn√≥stico exhaustivo que evalu√≥ 100 samples NONSPEECH limpios (sin degradaciones).

**Metodolog√≠a:**
- Modelo: Qwen2-Audio-7B-Instruct BASE (sin LoRA)
- Prompt: `"Classify this audio. Output only: SPEECH or NONSPEECH."`
- Samples: 100 NONSPEECH del test set
- An√°lisis: Respuestas RAW del modelo (antes de normalizaci√≥n)

**Resultados:**
```
Total samples NONSPEECH: 100
Correctos (NONSPEECH): 67 (67.00%)
Incorrectos (predice SPEECH): 33 (33.00%)
```

**Ejemplos de respuestas RAW:**
```
‚úó Sample 1 | GROUND TRUTH: NONSPEECH | RAW: 'SPEECH' | NORMALIZED: SPEECH
‚úó Sample 2 | GROUND TRUTH: NONSPEECH | RAW: 'SPEECH' | NORMALIZED: SPEECH
‚úó Sample 3 | GROUND TRUTH: NONSPEECH | RAW: 'SPEECH' | NORMALIZED: SPEECH
‚úì Sample 6 | GROUND TRUTH: NONSPEECH | RAW: 'NONSPEECH' | NORMALIZED: NONSPEECH
‚úì Sample 7 | GROUND TRUTH: NONSPEECH | RAW: 'NONSPEECH' | NORMALIZED: NONSPEECH
```

**Hallazgos clave:**
1. **El modelo responde literalmente "SPEECH"** cuando deber√≠a decir "NONSPEECH"
2. **El sistema de normalizaci√≥n funciona perfectamente** - no hay errores de interpretaci√≥n
3. **Confidence siempre es 1.0** - el modelo est√° muy seguro, aunque est√© equivocado
4. **Latencia sugiere sesgo:** SPEECH ~177ms vs NONSPEECH ~204ms (SPEECH es la respuesta "por defecto")

**Conclusi√≥n:** El problema es inherente al modelo BASE, NO es un problema de prompting ni de normalizaci√≥n.

---

### üéØ Hallazgos Clave del Diagn√≥stico

#### El Problema est√° en el MODELO BASE, NO en el Sistema de Normalizaci√≥n

**üìä Evidencia Experimental (100 samples NONSPEECH limpios):**
```
‚úÖ Correctos (NONSPEECH): 67 (67%)
‚ùå Incorrectos (predice SPEECH): 33 (33%)
```

**üîç An√°lisis de Respuestas RAW:**

El modelo est√° respondiendo **literalmente "SPEECH"** cuando deber√≠a decir "NONSPEECH":

```
‚úó Sample 1 | NORMALIZED: SPEECH     | RAW: raw_output='SPEECH', text='SPEECH'
‚úó Sample 2 | NORMALIZED: SPEECH     | RAW: raw_output='SPEECH', text='SPEECH'
‚úó Sample 3 | NORMALIZED: SPEECH     | RAW: raw_output='SPEECH', text='SPEECH'
‚úó Sample 4 | NORMALIZED: SPEECH     | RAW: raw_output='SPEECH', text='SPEECH'
‚úó Sample 5 | NORMALIZED: SPEECH     | RAW: raw_output='SPEECH', text='SPEECH'
‚úì Sample 6 | NORMALIZED: NONSPEECH  | RAW: raw_output='NONSPEECH', text='NONSPEECH'
‚úì Sample 7 | NORMALIZED: NONSPEECH  | RAW: raw_output='NONSPEECH', text='NONSPEECH'
```

**El sistema de normalizaci√≥n funciona PERFECTAMENTE:**
- Cuando el modelo dice "SPEECH" ‚Üí normaliza a SPEECH ‚úì
- Cuando el modelo dice "NONSPEECH" o "Nonspeech." ‚Üí normaliza a NONSPEECH ‚úì
- **NO HAY errores de interpretaci√≥n en el post-procesamiento**

**üí° Causa Ra√≠z Identificada:**

El modelo BASE (Qwen2-Audio-7B-Instruct sin fine-tuning) tiene un **SESGO INHERENTE** hacia clasificar audios como SPEECH:

1. **Sesgo sistem√°tico:** Sobre-predice SPEECH independientemente del prompt
2. **Empeora con degradaciones:**
   - Samples limpios: 33% error (67% accuracy)
   - Test set con degradaciones: 74.48% error (25.52% accuracy)
3. **Condiciones que agravan el sesgo:**
   - SNR bajo (-10dB, -5dB, 0dB)
   - Duraci√≥n muy corta (<100ms)
   - Filtros (bandpass, lowpass, highpass)
   - Reverberaci√≥n (0.3s, 1.0s, 2.5s)

**üî¨ Observaciones T√©cnicas:**

| M√©trica | Valor | Interpretaci√≥n |
|---------|-------|----------------|
| `confidence` | 1.0 (siempre) | Modelo muy seguro, incluso cuando se equivoca |
| Latencia SPEECH | ~177ms | Respuesta m√°s r√°pida (¬ørespuesta por defecto?) |
| Latencia NONSPEECH | ~204ms | Respuesta m√°s lenta (+15%) |
| `p_first_token` | Variable (0.44-0.87) | Sin patr√≥n claro de discriminaci√≥n |

**‚úÖ Por Qu√© LoRA Funciona:**

El fine-tuning con LoRA **corrige completamente este sesgo**:

| Modelo | NONSPEECH Accuracy | BA | Mejora |
|--------|-------------------|-----|--------|
| BASE sin LoRA | **25.52%** ‚ùå | 59.89% | - |
| LoRA fine-tuned | **87.64%** ‚úÖ | 92.99% | **+62.12 puntos** |

El fine-tuning entrena al modelo a:
- ‚úÖ NO sobre-predecir SPEECH por defecto
- ‚úÖ Detectar correctamente samples NONSPEECH
- ‚úÖ Mantener robustez bajo degradaciones severas

**üö´ Implicaciones:**

1. **NO tiene sentido optimizar prompts para el modelo BASE** - el sesgo es inherente al modelo pre-entrenado
2. **El prompting NO puede corregir este sesgo** - probamos 3 estrategias diferentes (auto, cl√°sico, varied) y todas fallan igual
3. **LoRA es ESENCIAL** - no opcional - para esta tarea espec√≠fica de detecci√≥n de habla

---

## Pipeline de 7 Etapas

1. **Evaluaci√≥n psicoac√∫stica (baseline)** - Establece la l√≠nea base
2. **LoRA finetuning** - Entrena adaptadores sobre el modelo base
3. **Evaluaci√≥n base vs LoRA** - Compara ambos modelos
4. **OPRO en modelo base** - Optimiza prompts para el modelo base
5. **OPRO en modelo LoRA** - Re-optimiza prompts para el modelo fine-tuned
6. **Evaluaci√≥n base + OPRO** - Eval con prompts optimizados
7. **Evaluaci√≥n LoRA + OPRO** - Eval final con LoRA + OPRO ‚≠ê

---

## Estructura del Proyecto

```
opro2_clean/
‚îú‚îÄ‚îÄ README.md                          # Este archivo
‚îú‚îÄ‚îÄ MANIFEST.md                        # Inventario completo de archivos
‚îú‚îÄ‚îÄ CLAUDE.md                          # Instrucciones para Claude Code
‚îú‚îÄ‚îÄ requirements.txt                   # Dependencias Python
‚îú‚îÄ‚îÄ config.yaml                        # Configuraci√≥n global
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ evaluate_simple.py             # Evaluaci√≥n principal (Etapas 1,3,6,7)
‚îÇ   ‚îú‚îÄ‚îÄ finetune_qwen_audio.py         # LoRA training (Etapa 2)
‚îÇ   ‚îú‚îÄ‚îÄ opro_classic_optimize.py       # OPRO cl√°sico (Etapa 4,5) ‚úÖ
‚îÇ   ‚îú‚îÄ‚îÄ opro_post_ft_v2.py             # OPRO post-FT
‚îÇ   ‚îú‚îÄ‚îÄ diagnose_base_nonspeech.py     # Diagn√≥stico de sesgo del modelo BASE
‚îÇ   ‚îú‚îÄ‚îÄ run_complete_pipeline.py       # Wrapper completo
‚îÇ   ‚îú‚îÄ‚îÄ run_opro_varied_complete.sh    # Pipeline OPRO con prompts variados
‚îÇ   ‚îî‚îÄ‚îÄ fix_base_opro_open.sh          # Fix para re-ejecutar BASE + OPRO
‚îÇ
‚îú‚îÄ‚îÄ slurm/                             # Jobs de SLURM
‚îÇ   ‚îú‚îÄ‚îÄ tools/on_submit.sh             # Wrapper para ejecutar comandos SLURM
‚îÇ   ‚îú‚îÄ‚îÄ opro_varied_base.job           # OPRO varied en BASE
‚îÇ   ‚îú‚îÄ‚îÄ opro_varied_lora.job           # OPRO varied en LoRA
‚îÇ   ‚îú‚îÄ‚îÄ eval_varied_base.job           # Evaluaci√≥n BASE + OPRO varied
‚îÇ   ‚îú‚îÄ‚îÄ eval_varied_lora.job           # Evaluaci√≥n LoRA + OPRO varied
‚îÇ   ‚îú‚îÄ‚îÄ diagnose_base.job              # Job de diagn√≥stico del sesgo BASE
‚îÇ   ‚îî‚îÄ‚îÄ *.job                          # Otros scripts de jobs
‚îÇ
‚îú‚îÄ‚îÄ src/qsm/                           # C√≥digo fuente
‚îÇ   ‚îú‚îÄ‚îÄ models/qwen_audio.py           # Wrapper del modelo
‚îÇ   ‚îî‚îÄ‚îÄ utils/normalize.py             # Utilidades de normalizaci√≥n
‚îÇ
‚îú‚îÄ‚îÄ prompts/                           # Archivos de prompts seed
‚îÇ   ‚îî‚îÄ‚îÄ open_descriptive_seeds.json   # 15 prompts variados para OPRO Varied
‚îÇ
‚îú‚îÄ‚îÄ checkpoints/                       # Checkpoints LoRA
‚îÇ   ‚îî‚îÄ‚îÄ qwen_lora_seed42/
‚îÇ       ‚îî‚îÄ‚îÄ final/                     # Checkpoint final usado en experimentos
‚îÇ
‚îú‚îÄ‚îÄ results/                           # Resultados de evaluaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ complete_pipeline_seed42/      # OPRO Cl√°sico ‚úÖ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 04_opro_base/              # BASE + OPRO Cl√°sico
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 05_opro_lora/              # LoRA + OPRO Cl√°sico (MEJOR: 94.90% BA)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ optimization_history.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ best_prompt.txt
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 06_eval_base_opro/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 07_eval_lora_opro/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ metrics.json
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ complete_pipeline_seed42_opro_open/  # OPRO Open ‚úÖ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 04_opro_base/              # BASE + OPRO auto (sesgo: 59.89% BA)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 05_opro_lora/              # LoRA + OPRO Open (94.78% BA)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 06_eval_base_opro/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 07_eval_lora_opro/
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ opro_varied_seed42/            # OPRO Varied (15 seeds diversos) ‚úÖ
‚îÇ       ‚îú‚îÄ‚îÄ base/                      # BASE + OPRO Varied (sesgo: 59.89% BA)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ optimization_history.json
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ best_prompt.txt
‚îÇ       ‚îú‚îÄ‚îÄ lora/                      # LoRA + OPRO Varied (92.99% BA)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ optimization_history.json
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ best_prompt.txt
‚îÇ       ‚îú‚îÄ‚îÄ eval_base/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ metrics.json
‚îÇ       ‚îî‚îÄ‚îÄ eval_lora/
‚îÇ           ‚îî‚îÄ‚îÄ metrics.json
‚îÇ
‚îî‚îÄ‚îÄ logs/                              # Logs de SLURM
    ‚îú‚îÄ‚îÄ diagnose_base_2028551.out      # Log del diagn√≥stico de sesgo BASE
    ‚îî‚îÄ‚îÄ *.out/*.err                    # Logs de jobs
```

---

## Requisitos

### Sistema
- Python >= 3.10
- CUDA >= 11.8
- **GPU:** 40GB+ VRAM para training, 24GB para inference (RTX 3090, A6000, V100)
- RAM: 48-64GB
- Disco: 100GB+ libres

### Instalaci√≥n

```bash
# Clonar repositorio
cd /mnt/fast/nobackup/users/gb0048/opro2_clean

# Instalar dependencias
pip install -r requirements.txt

# Verificar GPU
nvidia-smi
```

**Dependencias principales:**
- `torch>=2.0.0`, `torchaudio>=2.0.0`
- `transformers>=4.40.0`
- `peft>=0.10.0` (LoRA)
- `pandas>=2.0.0`, `pyarrow>=15.0.0`
- `librosa>=0.10.1`, `soundfile>=0.12.1`

---

## Uso en Surrey HPC

### Ejecutar Pipeline Completo

```bash
# V√≠a wrapper de submit (recomendado)
./slurm/tools/on_submit.sh sbatch slurm/00_run_complete_pipeline.job

# Ver cola de jobs
./slurm/tools/on_submit.sh squeue -u gb0048

# Ver detalles de un job
./slurm/tools/on_submit.sh scontrol show job JOBID

# Ver hist√≥rico
./slurm/tools/on_submit.sh sacct -j JOBID --format=JobID,State,ExitCode,Elapsed,ReqMem,MaxRSS
```

### Ejecutar Etapas Individuales

```bash
# Etapa 2: LoRA Training
./slurm/tools/on_submit.sh sbatch slurm/01_finetune_lora.job 42

# Etapa 5: OPRO en LoRA (cl√°sico - recomendado)
./slurm/tools/on_submit.sh sbatch slurm/03_opro_lora.job 42

# Etapa 7: Evaluaci√≥n final
./slurm/tools/on_submit.sh sbatch slurm/07_eval_lora_opro.job 42
```

---

## Configuraci√≥n T√©cnica

### LoRA

```yaml
lora:
  r: 64                     # Rank
  alpha: 16                 # Scaling
  dropout: 0.05
  task_type: CAUSAL_LM
  target_modules:
    - q_proj, k_proj, v_proj, o_proj
    - gate_proj, up_proj, down_proj
```

### Entrenamiento

- **Quantization:** 4-bit (QLoRA)
- **Batch size:** 2 √ó 4 gradient accumulation = 8 effective
- **Learning rate:** 5e-5
- **Epochs:** 3
- **Gradient checkpointing:** Enabled

### OPRO Cl√°sico (Recomendado)

- **Optimizer LLM:** Qwen/Qwen2.5-7B-Instruct (local)
- **Iterations:** 15
- **Samples per iteration:** 20
- **Candidates per iteration:** 8
- **Top-k memory:** 10 mejores prompts
- **Reward function:** Balanced Accuracy

### OPRO Varied (Experimental)

- **Optimizer LLM:** Qwen/Qwen2.5-7B-Instruct (local)
- **Iterations:** 15
- **Samples per iteration:** 30 (50% m√°s que Cl√°sico)
- **Candidates per iteration:** 8
- **Decoding mode:** open (permite respuestas libres)
- **Seed prompts:** 15 templates diversos (vs 8 en Cl√°sico)
- **Diversity strategy:** Incluye prompts descriptivos, binarios, con definiciones, con ejemplos, multiple choice, YES/NO
- **Resultado:** 92.99% BA en LoRA (excelente, 2% menor que Cl√°sico)

---

## Archivos de Resultados

### BASE + OPRO auto (Config 1) ‚ùå

- **Mejor prompt:** [results/complete_pipeline_seed42_opro_open/04_opro_base/best_prompt.txt](results/complete_pipeline_seed42_opro_open/04_opro_base/best_prompt.txt)
- **M√©tricas finales:** [results/complete_pipeline_seed42_opro_open/06_eval_base_opro/metrics.json](results/complete_pipeline_seed42_opro_open/06_eval_base_opro/metrics.json)
- **Status:** ‚ùå Sesgo inherente del modelo (59.89% BA)

### BASE + OPRO Cl√°sico (Config 2) ‚ö†Ô∏è

- **Historia de optimizaci√≥n:** [results/complete_pipeline_seed42/04_opro_base/optimization_history.json](results/complete_pipeline_seed42/04_opro_base/optimization_history.json)
- **Mejor prompt:** [results/complete_pipeline_seed42/04_opro_base/best_prompt.txt](results/complete_pipeline_seed42/04_opro_base/best_prompt.txt)
- **M√©tricas finales:** [results/complete_pipeline_seed42/06_eval_base_opro/metrics.json](results/complete_pipeline_seed42/06_eval_base_opro/metrics.json)
- **Status:** ‚ö†Ô∏è Resultado previo (88.12% BA) - inconsistente con experimentos recientes

### BASE + OPRO Varied (Config 3) ‚ùå

- **Historia de optimizaci√≥n:** [results/opro_varied_seed42/base/optimization_history.json](results/opro_varied_seed42/base/optimization_history.json)
- **Mejor prompt:** [results/opro_varied_seed42/base/best_prompt.txt](results/opro_varied_seed42/base/best_prompt.txt)
- **M√©tricas finales:** [results/opro_varied_seed42/eval_base/metrics.json](results/opro_varied_seed42/eval_base/metrics.json)
- **Status:** ‚ùå Sesgo inherente del modelo (59.89% BA)

### LoRA + OPRO Cl√°sico (Config 4) ‚≠ê MEJOR

- **Historia de optimizaci√≥n:** [results/complete_pipeline_seed42/05_opro_lora/optimization_history.json](results/complete_pipeline_seed42/05_opro_lora/optimization_history.json)
- **Mejor prompt:** [results/complete_pipeline_seed42/05_opro_lora/best_prompt.txt](results/complete_pipeline_seed42/05_opro_lora/best_prompt.txt)
- **M√©tricas finales:** [results/complete_pipeline_seed42/07_eval_lora_opro/metrics.json](results/complete_pipeline_seed42/07_eval_lora_opro/metrics.json)
- **Status:** ‚≠ê **94.90% BA** - Mejor resultado general

### LoRA + OPRO Open (Config 5) ‚úÖ

- **Historia de optimizaci√≥n:** [results/complete_pipeline_seed42_opro_open/05_opro_lora/optimization_history.json](results/complete_pipeline_seed42_opro_open/05_opro_lora/optimization_history.json)
- **Mejor prompt:** [results/complete_pipeline_seed42_opro_open/05_opro_lora/best_prompt.txt](results/complete_pipeline_seed42_opro_open/05_opro_lora/best_prompt.txt)
- **M√©tricas finales:** [results/complete_pipeline_seed42_opro_open/07_eval_lora_opro/metrics.json](results/complete_pipeline_seed42_opro_open/07_eval_lora_opro/metrics.json)
- **Status:** ‚úÖ 94.78% BA - Resultado casi id√©ntico a Cl√°sico

### LoRA + OPRO Varied (Config 6) ‚úÖ

- **Historia de optimizaci√≥n:** [results/opro_varied_seed42/lora/optimization_history.json](results/opro_varied_seed42/lora/optimization_history.json)
- **Mejor prompt:** [results/opro_varied_seed42/lora/best_prompt.txt](results/opro_varied_seed42/lora/best_prompt.txt)
- **M√©tricas finales:** [results/opro_varied_seed42/eval_lora/metrics.json](results/opro_varied_seed42/eval_lora/metrics.json)
- **Status:** ‚úÖ 92.99% BA - Prompts m√°s diversos, resultado excelente

---

## Troubleshooting

### Error: "CUDA out of memory"
```bash
# Reducir batch size
python scripts/evaluate_simple.py --batch_size 20  # default: 50

# Configurar memoria expandible
export PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"
```

### Error: "Checkpoint not found"
```bash
# Verificar que existe el checkpoint
ls -la checkpoints/qwen_lora_seed42/final/

# Si no existe, entrenar primero
./slurm/tools/on_submit.sh sbatch slurm/01_finetune_lora.job 42
```

### Jobs con DependencyNeverSatisfied
```bash
# Ver detalles del job
./slurm/tools/on_submit.sh scontrol show job JOBID | sed -n '1,120p'

# Cancelar y reenviar sin dependencia
./slurm/tools/on_submit.sh scancel JOBID
./slurm/tools/on_submit.sh sbatch slurm/script.job
```

---

## Documentaci√≥n Adicional

- **[CLAUDE.md](CLAUDE.md):** Reglas operativas para Claude Code en Surrey HPC
- **[MANIFEST.md](MANIFEST.md):** Inventario completo de archivos
- **[config.yaml](config.yaml):** Configuraci√≥n global del proyecto
- **[RUN_PIPELINE.md](RUN_PIPELINE.md):** Gu√≠a detallada de ejecuci√≥n

---

## Pr√≥ximos Pasos

### Investigaciones Recomendadas

1. **Validaci√≥n y Reproducibilidad:**
   - ‚úÖ **COMPLETADO:** Sesgo del modelo BASE documentado y diagnosticado
   - Probar con diferentes seeds (43, 44, 45) para validar reproducibilidad de LoRA + OPRO
   - Evaluar estabilidad de LoRA training con diferentes random seeds

2. **Optimizaciones de LoRA:**
   - Experimentar con diferentes configuraciones de LoRA (r=32, r=128, r=256)
   - Probar diferentes learning rates (1e-5, 1e-4)
   - Evaluar impacto de m√°s epochs de training (5, 10)
   - **Baseline importante:** Evaluar LoRA SIN OPRO para cuantificar beneficio puro de la optimizaci√≥n de prompts

3. **An√°lisis de Errores en LoRA:**
   - **Investigar por qu√© duration corta (<100ms) tiene peor rendimiento** (80-90% BA)
     - Hip√≥tesis: Clips muy cortos no proveen suficiente contexto temporal
     - Posible soluci√≥n: Prompt especializado o data augmentation
   - Analizar las 1,732 muestras mal clasificadas en NONSPEECH (8.43% error con mejor modelo)
   - Estudiar si hay patrones en los errores por condici√≥n psicoac√∫stica

4. **Experimentos OPRO Avanzados (solo con LoRA):**
   - Probar otros LLMs optimizadores (Llama 3, Mistral, GPT-4)
   - Experimentar con m√°s iteraciones (20, 30)
   - Probar con m√°s samples por iteraci√≥n (40, 50)
   - Evaluar si OPRO multi-objetivo (maximizar BA + minimizar latencia) mejora eficiencia

5. **NO Recomendado:**
   - ‚ùå Optimizar prompts para modelo BASE sin LoRA (sesgo inherente no corregible)
   - ‚ùå Intentar otras estrategias de prompting en BASE (ya probamos 3, todas fallan igual)

---

## Contacto

**Proyecto:** OPRO2 - Optimizaci√≥n de Prompts para Detecci√≥n de Habla
**Ubicaci√≥n:** Surrey HPC (aisurrey-submit01.surrey.ac.uk)
**Working Directory:** `/mnt/fast/nobackup/users/gb0048/opro2_clean`

Para preguntas o problemas:
1. Revisar logs en `logs/`
2. Consultar `CLAUDE.md` para comandos SLURM
3. Verificar estado de jobs con `./slurm/tools/on_submit.sh squeue -u gb0048`

---

**√öltima actualizaci√≥n:** 26 de diciembre 2025
**Versi√≥n:** 4.0
**Status:** üü¢ Todos los experimentos completados | ‚úÖ Sesgo del modelo BASE diagnosticado | ‚≠ê LoRA + OPRO validado como mejor configuraci√≥n (94.90% BA)
