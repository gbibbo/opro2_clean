# OPRO2 - Optimizaci√≥n de Prompts para Detecci√≥n de Habla

Pipeline completo de optimizaci√≥n de prompts (OPRO) para detecci√≥n de habla con Qwen2-Audio y LoRA en Surrey HPC.

---

## Descripci√≥n

Este repositorio implementa un pipeline de 7 etapas para optimizar la detecci√≥n de habla mediante:
- **OPRO (Optimization by PROmpting)**: Optimizaci√≥n autom√°tica de prompts usando un LLM local
- **LoRA (Low-Rank Adaptation)**: Fine-tuning eficiente del modelo Qwen2-Audio-7B-Instruct
- **Evaluaci√≥n psicoac√∫stica**: Medici√≥n de rendimiento bajo 22 condiciones independientes

---

## √öltimos Resultados Experimentales

### ‚ö†Ô∏è Correcci√≥n Importante (22 diciembre 2025)

El experimento inicial "OPRO Open Prompts" que report√≥ 0% accuracy NO fall√≥ debido a un problema con los prompts abiertos. El problema real fue **infraestructura: CUDA driver initialization failed** dentro del contenedor Apptainer en el nodo aisurrey14. Todas las 21,340 predicciones obtuvieron error de CUDA, no problemas de formato de prompt.

**Soluci√≥n implementada:**
- Excluir nodos problem√°ticos (aisurrey14, aisurrey19)
- Agregar test de CUDA antes de ejecutar
- Re-ejecutar stages fallidos individualmente

**Resultado:** Tras corregir el problema de infraestructura, OPRO Open Prompts funciona correctamente y obtiene resultados pr√°cticamente id√©nticos a OPRO Cl√°sico.

---

### Tabla Comparativa Completa de Configuraciones

**Test Set:** 21,340 muestras | **Seed:** 42 | **Fecha:** 15-22 diciembre 2025

| Configuraci√≥n | BA_clip | BA_conditions | Speech Acc | Nonspeech Acc | Prompt Optimizado |
|--------------|---------|---------------|------------|---------------|-------------------|
| **1. BASE + Prompt Base** | - | - | - | - | *No disponible* |
| **2. BASE + OPRO Cl√°sico** | 88.12% | 89.34% | 91.64% | 84.60% | Ver abajo ¬π |
| **3. BASE + OPRO Open** | ‚è≥ | ‚è≥ | ‚è≥ | ‚è≥ | *En ejecuci√≥n (Job 2027437)* |
| **4. LoRA + Prompt Base** | - | - | - | - | *No disponible* |
| **5. LoRA + OPRO Cl√°sico** | **94.90%** ‚≠ê | **95.46%** ‚≠ê | **98.23%** | **91.57%** | Ver abajo ¬≤ |
| **6. LoRA + OPRO Open** | **94.78%** ‚úÖ | **95.32%** ‚úÖ | **98.23%** | **91.34%** | Ver abajo ¬≥ |

**Diferencias clave:**
- **BASE ‚Üí LoRA:** +6.66-6.78% BA (beneficio del fine-tuning)
- **OPRO Cl√°sico vs Open:** Diferencia m√≠nima (0.12% BA) - pr√°cticamente id√©nticos
- **Mejor configuraci√≥n:** LoRA + OPRO (Cl√°sico u Open) con ~95% BA

---

### Prompts Optimizados por OPRO

**¬π Mejor Prompt - BASE + OPRO Cl√°sico:**
```
Listen briefly; is this clip human speech or noise? Quickly reply: SPEECH or NON-SPEECH.
```

**¬≤ Mejor Prompt - LoRA + OPRO Cl√°sico:**
```
Decide the dominant content.
Definitions:
- SPEECH = human voice, spoken words, syllables, conversational cues.
- NONSPEECH = music, tones/beeps, environmental noise, silence.
Output exactly: SPEECH or NONSPEECH.
```

**¬≥ Mejor Prompt - LoRA + OPRO Open:**
```
Decide the dominant content.
Definitions:
- SPEECH = human voice, spoken words, syllables, conversational cues.
- NONSPEECH = music, tones/beeps, environmental noise, silence.
Output exactly: SPEECH or NONSPEECH.
```

**Observaci√≥n:** OPRO Cl√°sico y OPRO Open convergieron al **mismo prompt id√©ntico** para el modelo LoRA, explicando por qu√© obtienen resultados casi iguales.

---

### An√°lisis Detallado: LoRA + OPRO Cl√°sico (Mejor Resultado)

**Desglose por Dimensi√≥n Psicoac√∫stica:**
| Dimensi√≥n | BA | Condiciones |
|-----------|-----|------------|
| Duration | 89.14% | 8 condiciones (20ms-1000ms) |
| SNR | 97.11% | 6 condiciones (-10dB a 20dB) |
| Reverb | 93.71% | 4 condiciones (none, 0.3s, 1.0s, 2.5s) |
| Filter | 93.94% | 4 condiciones (none, bandpass, lowpass, highpass) |

**Rendimiento por Condici√≥n (Top 5):**
1. SNR 5dB: 97.32% BA
2. SNR 10dB: 97.22% BA
3. SNR -10dB/20dB/-5dB/0dB: ~97.01% BA
4. Filter Bandpass: 94.33% BA
5. Filter Lowpass: 94.12% BA

**Rendimiento por Condici√≥n (Bottom 5):**
1. Duration 20ms: 80.93% BA
2. Duration 40ms: 84.85% BA
3. Duration 60ms: 87.32% BA
4. Duration 80ms: 88.04% BA
5. Duration 100ms: 90.82% BA

**Evoluci√≥n del OPRO:**
- Iteraci√≥n 1: 90% accuracy (prompt inicial)
- Mejor iteraci√≥n: Iter 76 con 100% accuracy en muestra de validaci√≥n
- Total de prompts generados: 121 (15 iteraciones √ó 8 candidatos)
- Top prompts recurrentes:
  - "Does this audio contain human speech? Answer exactly one token: SPEECH or NONSPEECH." ‚Üí 95%
  - "Decide the dominant content..." ‚Üí 100% (mejor)
  - "Label SPEECH only if human voice is clearly present..." ‚Üí 95%

---

### Conclusiones

1. **LoRA es esencial:** El fine-tuning mejora +6.66% BA sobre el modelo base
2. **OPRO funciona muy bien:** Optimizaci√≥n de prompts mejora significativamente el rendimiento
3. **OPRO Cl√°sico ‚âà OPRO Open:** No hay diferencia pr√°ctica (0.12% BA), ambos convergen al mismo prompt
4. **Duraci√≥n corta es el desaf√≠o:** Clips <100ms tienen peor rendimiento (80-90% BA)
5. **SNR muy robusto:** Excelente rendimiento incluso a -10dB (97% BA)

---

## Pipeline de 7 Etapas

1. **Evaluaci√≥n psicoac√∫stica (baseline)** - Establece la l√≠nea base
2. **LoRA finetuning** - Entrena adaptadores sobre el modelo base
3. **Evaluaci√≥n base vs LoRA** - Compara ambos modelos
4. **OPRO en modelo base** - Optimiza prompts para el modelo base
5. **OPRO en modelo LoRA** - Re-optimiza prompts para el modelo fine-tuned
6. **Evaluaci√≥n base + OPRO** - Eval con prompts optimizados
7. **Evaluaci√≥n LoRA + OPRO** - Eval final con LoRA + OPRO ‚≠ê

---

## Estructura del Proyecto

```
opro2_clean/
‚îú‚îÄ‚îÄ README.md                          # Este archivo
‚îú‚îÄ‚îÄ MANIFEST.md                        # Inventario completo de archivos
‚îú‚îÄ‚îÄ CLAUDE.md                          # Instrucciones para Claude Code
‚îú‚îÄ‚îÄ requirements.txt                   # Dependencias Python
‚îú‚îÄ‚îÄ config.yaml                        # Configuraci√≥n global
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ evaluate_simple.py             # Evaluaci√≥n principal (Etapas 1,3,6,7)
‚îÇ   ‚îú‚îÄ‚îÄ finetune_qwen_audio.py         # LoRA training (Etapa 2)
‚îÇ   ‚îú‚îÄ‚îÄ opro_classic_optimize.py       # OPRO cl√°sico (Etapa 4,5) ‚úÖ
‚îÇ   ‚îú‚îÄ‚îÄ opro_post_ft_v2.py             # OPRO post-FT
‚îÇ   ‚îú‚îÄ‚îÄ opro_open_prompts.py           # OPRO open prompts ‚ùå
‚îÇ   ‚îî‚îÄ‚îÄ run_complete_pipeline.py       # Wrapper completo
‚îÇ
‚îú‚îÄ‚îÄ slurm/                             # Jobs de SLURM
‚îÇ   ‚îú‚îÄ‚îÄ tools/on_submit.sh             # Wrapper para ejecutar comandos SLURM
‚îÇ   ‚îî‚îÄ‚îÄ *.job                          # Scripts de jobs
‚îÇ
‚îú‚îÄ‚îÄ src/qsm/                           # C√≥digo fuente
‚îÇ   ‚îú‚îÄ‚îÄ models/qwen_audio.py           # Wrapper del modelo
‚îÇ   ‚îî‚îÄ‚îÄ utils/normalize.py             # Utilidades de normalizaci√≥n
‚îÇ
‚îú‚îÄ‚îÄ checkpoints/                       # Checkpoints LoRA
‚îÇ   ‚îî‚îÄ‚îÄ qwen_lora_seed42/
‚îÇ       ‚îî‚îÄ‚îÄ final/                     # Checkpoint final usado en experimentos
‚îÇ
‚îú‚îÄ‚îÄ results/                           # Resultados de evaluaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ complete_pipeline_seed42/      # Experimento 1 ‚úÖ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 05_opro_lora/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ optimization_history.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ best_prompt.txt
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 03_eval_lora/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ metrics.json
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ complete_pipeline_seed42_opro_open/  # Experimento 2 ‚ùå
‚îÇ       ‚îú‚îÄ‚îÄ 05_opro_lora/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ optimization_history.json
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ best_prompt.txt
‚îÇ       ‚îî‚îÄ‚îÄ 07_eval_lora_opro/
‚îÇ           ‚îî‚îÄ‚îÄ metrics.json
‚îÇ
‚îî‚îÄ‚îÄ logs/                              # Logs de SLURM
```

---

## Requisitos

### Sistema
- Python >= 3.10
- CUDA >= 11.8
- **GPU:** 40GB+ VRAM para training, 24GB para inference (RTX 3090, A6000, V100)
- RAM: 48-64GB
- Disco: 100GB+ libres

### Instalaci√≥n

```bash
# Clonar repositorio
cd /mnt/fast/nobackup/users/gb0048/opro2_clean

# Instalar dependencias
pip install -r requirements.txt

# Verificar GPU
nvidia-smi
```

**Dependencias principales:**
- `torch>=2.0.0`, `torchaudio>=2.0.0`
- `transformers>=4.40.0`
- `peft>=0.10.0` (LoRA)
- `pandas>=2.0.0`, `pyarrow>=15.0.0`
- `librosa>=0.10.1`, `soundfile>=0.12.1`

---

## Uso en Surrey HPC

### Ejecutar Pipeline Completo

```bash
# V√≠a wrapper de submit (recomendado)
./slurm/tools/on_submit.sh sbatch slurm/00_run_complete_pipeline.job

# Ver cola de jobs
./slurm/tools/on_submit.sh squeue -u gb0048

# Ver detalles de un job
./slurm/tools/on_submit.sh scontrol show job JOBID

# Ver hist√≥rico
./slurm/tools/on_submit.sh sacct -j JOBID --format=JobID,State,ExitCode,Elapsed,ReqMem,MaxRSS
```

### Ejecutar Etapas Individuales

```bash
# Etapa 2: LoRA Training
./slurm/tools/on_submit.sh sbatch slurm/01_finetune_lora.job 42

# Etapa 5: OPRO en LoRA (cl√°sico - recomendado)
./slurm/tools/on_submit.sh sbatch slurm/03_opro_lora.job 42

# Etapa 7: Evaluaci√≥n final
./slurm/tools/on_submit.sh sbatch slurm/07_eval_lora_opro.job 42
```

---

## Configuraci√≥n T√©cnica

### LoRA

```yaml
lora:
  r: 64                     # Rank
  alpha: 16                 # Scaling
  dropout: 0.05
  task_type: CAUSAL_LM
  target_modules:
    - q_proj, k_proj, v_proj, o_proj
    - gate_proj, up_proj, down_proj
```

### Entrenamiento

- **Quantization:** 4-bit (QLoRA)
- **Batch size:** 2 √ó 4 gradient accumulation = 8 effective
- **Learning rate:** 5e-5
- **Epochs:** 3
- **Gradient checkpointing:** Enabled

### OPRO Cl√°sico (Recomendado)

- **Optimizer LLM:** Qwen/Qwen2.5-7B-Instruct (local)
- **Iterations:** 15
- **Samples per iteration:** 20
- **Candidates per iteration:** 8
- **Top-k memory:** 10 mejores prompts
- **Reward function:** Balanced Accuracy

---

## Archivos de Resultados

### BASE + OPRO Cl√°sico

- **Historia de optimizaci√≥n:** [results/complete_pipeline_seed42/04_opro_base/optimization_history.json](results/complete_pipeline_seed42/04_opro_base/optimization_history.json) *(parcial)*
- **Mejor prompt:** [results/complete_pipeline_seed42/04_opro_base/best_prompt.txt](results/complete_pipeline_seed42/04_opro_base/best_prompt.txt)
- **M√©tricas finales:** [results/complete_pipeline_seed42/06_eval_base_opro/metrics.json](results/complete_pipeline_seed42/06_eval_base_opro/metrics.json)

### BASE + OPRO Open

- **Historia de optimizaci√≥n:** [results/complete_pipeline_seed42_opro_open/04_opro_base/optimization_history.json](results/complete_pipeline_seed42_opro_open/04_opro_base/optimization_history.json) *(en ejecuci√≥n)*
- **Mejor prompt:** [results/complete_pipeline_seed42_opro_open/04_opro_base/best_prompt.txt](results/complete_pipeline_seed42_opro_open/04_opro_base/best_prompt.txt) *(pendiente)*
- **M√©tricas finales:** [results/complete_pipeline_seed42_opro_open/06_eval_base_opro/metrics.json](results/complete_pipeline_seed42_opro_open/06_eval_base_opro/metrics.json) *(pendiente)*

### LoRA + OPRO Cl√°sico (Mejor configuraci√≥n)

- **Historia de optimizaci√≥n:** [results/complete_pipeline_seed42/05_opro_lora/optimization_history.json](results/complete_pipeline_seed42/05_opro_lora/optimization_history.json)
- **Mejor prompt:** [results/complete_pipeline_seed42/05_opro_lora/best_prompt.txt](results/complete_pipeline_seed42/05_opro_lora/best_prompt.txt)
- **M√©tricas finales:** [results/complete_pipeline_seed42/07_eval_lora_opro/metrics.json](results/complete_pipeline_seed42/07_eval_lora_opro/metrics.json)

### LoRA + OPRO Open (Resultado casi id√©ntico)

- **Historia de optimizaci√≥n:** [results/complete_pipeline_seed42_opro_open/05_opro_lora/optimization_history.json](results/complete_pipeline_seed42_opro_open/05_opro_lora/optimization_history.json)
- **Mejor prompt:** [results/complete_pipeline_seed42_opro_open/05_opro_lora/best_prompt.txt](results/complete_pipeline_seed42_opro_open/05_opro_lora/best_prompt.txt)
- **M√©tricas finales:** [results/complete_pipeline_seed42_opro_open/07_eval_lora_opro/metrics.json](results/complete_pipeline_seed42_opro_open/07_eval_lora_opro/metrics.json)

---

## Troubleshooting

### Error: "CUDA out of memory"
```bash
# Reducir batch size
python scripts/evaluate_simple.py --batch_size 20  # default: 50

# Configurar memoria expandible
export PYTORCH_CUDA_ALLOC_CONF="expandable_segments:True"
```

### Error: "Checkpoint not found"
```bash
# Verificar que existe el checkpoint
ls -la checkpoints/qwen_lora_seed42/final/

# Si no existe, entrenar primero
./slurm/tools/on_submit.sh sbatch slurm/01_finetune_lora.job 42
```

### Jobs con DependencyNeverSatisfied
```bash
# Ver detalles del job
./slurm/tools/on_submit.sh scontrol show job JOBID | sed -n '1,120p'

# Cancelar y reenviar sin dependencia
./slurm/tools/on_submit.sh scancel JOBID
./slurm/tools/on_submit.sh sbatch slurm/script.job
```

---

## Documentaci√≥n Adicional

- **[CLAUDE.md](CLAUDE.md):** Reglas operativas para Claude Code en Surrey HPC
- **[MANIFEST.md](MANIFEST.md):** Inventario completo de archivos
- **[config.yaml](config.yaml):** Configuraci√≥n global del proyecto
- **[RUN_PIPELINE.md](RUN_PIPELINE.md):** Gu√≠a detallada de ejecuci√≥n

---

## Pr√≥ximos Pasos

### Investigaciones Pendientes

1. **Optimizaciones Posibles:**
   - Probar con diferentes seeds (43, 44, 45) para validar reproducibilidad
   - Experimentar con diferentes configuraciones de LoRA (r=32, r=128)
   - Probar otros LLMs para OPRO (Llama, Mistral)
   - Evaluar con prompt base (sin OPRO) para cuantificar beneficio de optimizaci√≥n

2. **An√°lisis de Errores:**
   - **Investigar por qu√© duration corta (<100ms) tiene peor rendimiento**
     - Hip√≥tesis: Clips muy cortos no proveen suficiente contexto temporal
     - Posible soluci√≥n: Prompt especializado para duraciones cortas
   - Analizar muestras mal clasificadas en nonspeech (8.43% error)
   - Estudiar confusiones espec√≠ficas por condici√≥n

3. **Infraestructura:**
   - ‚úÖ **RESUELTO:** Problema de CUDA en nodos aisurrey14/aisurrey19
   - Documentar nodos confiables para futuros experimentos
   - Considerar migrar a contenedor actualizado si persisten problemas

---

## Contacto

**Proyecto:** OPRO2 - Optimizaci√≥n de Prompts para Detecci√≥n de Habla
**Ubicaci√≥n:** Surrey HPC (aisurrey-submit01.surrey.ac.uk)
**Working Directory:** `/mnt/fast/nobackup/users/gb0048/opro2_clean`

Para preguntas o problemas:
1. Revisar logs en `logs/`
2. Consultar `CLAUDE.md` para comandos SLURM
3. Verificar estado de jobs con `./slurm/tools/on_submit.sh squeue -u gb0048`

---

**√öltima actualizaci√≥n:** 22 de diciembre 2025
**Versi√≥n:** 3.0
**Status:** üü¢ Pipeline completo funcional | ‚úÖ OPRO Cl√°sico y Open Prompts validados | ‚è≥ Completando experimentos BASE + OPRO Open
